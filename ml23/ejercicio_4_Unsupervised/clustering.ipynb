{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de agrupamiento\n",
    "\n",
    "En este ejercicio, aplicaremos los conceptos vistos en clase referentes a métodos de agrupamiento no supervisados. Específicamente, explorarás el uso de KMeans y DBScan usando la librería de scikit-learn. En la primera sección analizaremos K-means y en la segunda DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "random_state = 0 # Fijamos la semilla aleatoria para que siempre den los mismos datos\n",
    "np.random.seed(random_state)\n",
    "# Podemos establecer los colores con los que se graficará con matplotlib\n",
    "# utilizando un cmap distinto\n",
    "matplotlib.rc('image', cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaremos 3 blobs, cada uno con datos iguales. En total habran 1000 puntos distribuidos en estos blobs.\n",
    "n_samples = 1000\n",
    "X, _ = datasets.make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "\n",
    "# Utilizaremos el algoritmo de KMeans para encontrar clusters en los datos\n",
    "n_cols = 4\n",
    "n_clusters_list = [2, 3, 4, 5]\n",
    "rows = len(n_clusters_list) // n_cols\n",
    "fig, axes = plt.subplots(rows, n_cols, figsize=(4*n_cols, 4))\n",
    "axes = axes.flatten()\n",
    "for ax, n_clusters in zip(axes, n_clusters_list):\n",
    "    # TODO: Encuentra los clusters utilizando el algoritmo de KMeans implementado en scikit-learn\n",
    "    # ====== Start of solution =====\n",
    "    model = ...\n",
    "\n",
    "    # TODO: Grafica los datos usando scatter, recuerda que X = (x, y)\n",
    "\n",
    "    # TODO: Lee la documentación de sklearn para obtener los centroides del modelo entrenado\n",
    "    # y grafícalos con ax.scatter\n",
    "\n",
    "\n",
    "    ax.set_title(f\"K-means: {n_clusters} clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sección anterior utilizamos 2, 3, 4 y 5 clusters para encontar patrones en los datos y los resultados se graficaron.\n",
    "\n",
    "TODO: Contesta la siguiente pregunta\n",
    "\n",
    "- ¿Cúal propuesta de clusters consideras la mejor y porqué?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Definiendo métricas de evaluación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el algoritmo de K-means debemos seleccionar la cantidad de clusters para poder hacer fit al modelo. Para decidir cuantos clusters utilizar podemos evaluar visualmente o hacer un análisis cuantitativo. \n",
    "\n",
    "Una estrategia común para realizar el análisis consiste en entrenar modelos con multiples propuestas de k-grupos y seleccionar la propuesta con el mejor rendimiento. Para ello se debe utilizar una métrica que evalúe el desempeño del algoritmo.\n",
    "\n",
    "El [\"silhouette score\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) es una métrica muy útil para este objetivo que consiste en medir la distancia intra-grupo e inter-grupo. Esta métrica considera como buen grupo a aquel que tiene una pequeña distancia promedio intra-grupo y que se encuentra muy separado de los demas grupos. Un acomodo con buenos grupos tendrá un silhoute score cercano a 1 y una mala selección de grupos tendrá un score cercano -1. \n",
    "\n",
    "En la siguiente sección seguiremos esta estrategia para determinar la mejor cantidad de grupos en los datos provistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "Ks = range(2,6)\n",
    "scores = []\n",
    "for k in Ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "    # TODO: Obten el silhouete score de nuestro modelo\n",
    "    # ====== Start of solution =====\n",
    "    score = ...\n",
    "    # ====== End of solution =====\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(Ks,scores,'-o', markersize=8)\n",
    "plt.xlabel(\"valor de K\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Contesta la siguiente pregunta\n",
    "\n",
    "De acuerdo al silhouete score\n",
    "- ¿Qué sería mejor, elegir 4 o 5 clusters?\n",
    "- ¿Cuántos clusters se deberían utilizar para entrenar el algoritmo? ¿Concuerda con tu selección realizada tras visualizar los datos?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Rompiendo las asunciones de K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means es un algoritmo que funciona en base a algunas asunciones que pueden no siempre ser ciertas, en esta seccion analizaremos algunos casos en los cuales el K-means puede que no genere la solución esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "# =============================================================================\n",
    "# Generamos diferentes conjuntos de datos que rompen las asunciones de K-Means\n",
    "# =============================================================================\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "# Blobs con varianzas variadas\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# Cada dataset es un tuple (datos, etiquetas)\n",
    "datasets = {\"Círculos\": noisy_circles,\n",
    "            \"Lunas\": noisy_moons, \n",
    "            \"Blobs\": blobs, \n",
    "            \"Varianzas distintas\": varied,\n",
    "            # \"Aleatorio\": no_structure,\n",
    "            \"Datos anisotrópicos\": aniso}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Graficamos los datos\n",
    "# ============\n",
    "n_rows = 1\n",
    "n_cols = len(datasets)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2))\n",
    "axs = axs.flatten()\n",
    "for (name, data), ax in zip(datasets.items(), axs):\n",
    "    X, _ = data\n",
    "    # Normalizamos los datos para que todos estén en la misma escala\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    # TODO: Grafica los datos usando scatter\n",
    "    # ====== Start of solution =====\n",
    "\n",
    "    # ====== End of solution =====\n",
    "    ax.set_title(name)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para los conjuntos de datos anteriores...\n",
    "Aplica K-means a cada uno de los datasets anteriores. Determina los hiperparámetros (k) que resulten en la mejor asignación según tu criterio para cada grupo.\n",
    "- ¿En que conjunto/os esperas que k-means realice una solución adecuada?\n",
    "- ¿Cuales son las asunciones de k-means?\n",
    "- ¿Cual de estos conjuntos NO rompe las asunciones de k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1\n",
    "n_cols = len(datasets)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2))\n",
    "axs = axs.flatten()\n",
    "# TODO: Modifica la cantidad de clusters para cada dataset hasta que encuentres un resultado que te satisfaga\n",
    "n_clusters = {\"Círculos\": 1,\n",
    "              \"Lunas\": 1,\n",
    "              \"Blobs\": 1,\n",
    "              \"Varianzas distintas\": 1,\n",
    "              \"Anisotrópicos\": 1}\n",
    "for (name, data), n, ax in zip(datasets.items(), n_clusters.values(), axs):\n",
    "    X, _ = data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    ax.set_title(name)\n",
    "    # TODO: Aplica K-means para encontrar los grupos y los centros de cada cluster\n",
    "    # Después modifica los hiperparámetros en n_clusters según consideres apropiado para cada dataset\n",
    "    # ====== Start of solution =====\n",
    "\n",
    "    centers = ...\n",
    "    # ====== End of solution =====\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=10, c=kmeans.labels_, vmin=-1)\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], marker=\"+\", s=30, c='k')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DBScan\n",
    "DBScan es otro método de agrupamiento el cual encuentra grupos que funciona a través de densidad local.\n",
    "En las siguientes celdas, aplica DBScan a cada uno de los datasets anteriores y compara los resultados con los de K-Means. Ajusta los hiperparámetros según tu criterio para lograr un agrupamiento adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1\n",
    "n_cols = len(datasets)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2))\n",
    "axs = axs.flatten()\n",
    "# TODO: Modifica los hiper parámetros de dbscan hasta encontrar un resultado que te satisfaga\n",
    "params = {\"Círculos\": {\"eps\": 0.1, \"min_samples\": 1},\n",
    "          \"Lunas\": {\"eps\": 0.1, \"min_samples\": 1},\n",
    "          \"Blobs\": {\"eps\": 0.1, \"min_samples\": 1},\n",
    "          \"Varianzas distintas\": {\"eps\": 0.1, \"min_samples\": 1},\n",
    "          \"Datos anisotrópicos\": {\"eps\": 0.1, \"min_samples\": 1}}\n",
    "for ((name, data), ax) in zip(datasets.items(), axs):\n",
    "    X, _ = data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    ax.set_title(name)\n",
    "    eps = params[name]['eps']\n",
    "    min_samples = params[name]['min_samples']\n",
    "    # TODO: Aplica DBSCAN para encontrar los clusters\n",
    "    # Después modifica los hiperparámetros en n_clusters según consideres apropiado para cada dataset\n",
    "    # ====== Start of solution =====\n",
    "    grupos = ...\n",
    "    # ====== End of solution =====\n",
    "    print(f\"Clusteres en {name}\", np.unique(grupos))\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=10, c=grupos, vmin=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Contesta la siguientes preguntas\n",
    "- ¿Qué diferencia encuentras entre los clusters encontrados con DBSCAN y los clusters encontrados con K-Means?\n",
    "- ¿Qué metodo clasifica mejor los clusters de Lunas y a qué crees que se deba?\n",
    "- ¿Qué son los puntos negros encontrados con DBSCAN?\n",
    "- ¿En qué casos crees que sea recomendable utilizar K-means sobre DBSCAN?\n",
    "- En base a tu experiencia ¿que desventajas notas para DBScan contra K-means?\n",
    "- En base a tu experiencia ¿que ventajas notas para DBScan contra K-means?\n",
    "- Si entrenamos DBSCAN y queremos buscar un grupo al que pertenezca un punto nuevo, ¿Existe alguna función de sk-learn que nos permita hacer esto?¿Cómo podríamos predecir el grupo al que pertenece un nuevo datapoint?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistemas_inteligentes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68aa2b113ed54e18612038c0cfcfc7992ada4214d291a0318c102acd42c122d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
