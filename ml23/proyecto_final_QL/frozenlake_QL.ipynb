{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Proyecto Final ML23"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","import gymnasium as gym\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["# **Parametros**\n","\n","* `interacion:`       **int**   #Episodios\n","* `learning_rate:`    **float** #Learning rate\n","* `gamma:`            **float** #Discount rate\n","* `epsilon:`          **float** # Exploration probability\n","* `seed:`             **int**   # Define a seed so that we get reproducible results\n","* `n_runs:`           **int**   # Number of runs\n","* `action_size:`      **int**   # Number of possible actions\n","* `state_size:`       **int**   # Number of possible states\n","* `proba_frozen:`     **float** # Probability that a tile is frozen\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed = \"waf\"\n","rng = np.random.default(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Qlearning:\n","    def _init_(self, learning_rate, gamma, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.learning_rate = learning_rate\n","        self.gamma = gamma\n","        self.reset_qtable()\n","    \n","    def update(self, state, action, reward, new_state):\n","        #Update Q(s,a) = Q(s,a) + Lr[ R(s,a) + gamma * maxQ(s',a') - Q(s,a)]\n","        delta = (\n","            reward\n","            + self.gamma *np.max(self.qtable[new_state, :])\n","            - self.qtable[state, action]\n","        )\n","        q_update = self.qtable[state, action] + self.learning_rate * delta\n","        return q_update\n","    \n","    def reset_qtable(self):\n","        #Resets Q-Table\n","        self.qtable=np.zeros((self.state_size,self.action_size))\n","\n","class EpsilonGreedy:\n","    def _init_(self, epsilon):\n","        self.epsilon = epsilon\n","\n","    def choose_action(self, action_space, state, qtable):\n","        \"Escoger una accion 'a' en el contexto actual (s)\"\n","        #Randomiza un numero\n","        explore_exploit_tradeoff = rng.uniform(0,1)\n","\n","        #Exploracion\n","        if(explore_exploit_tradeoff<self.epsilon):\n","            action = action_space.sample()\n","        \n","        #Explotacion (tomar el Q-valor mas grande para este estado)\n","        else:\n","            if np.all(qtable[state, :]) == qtable [state, 0]:\n","                action = action_space.sample()\n","            else:\n","                action = np.argmax(qtable[state,:])\n","        return action\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Interacion numero: 1 hizo la accion numero: 1\n","(4, 0.0, False, False, {'prob': 1.0})\n","Interacion numero: 2 hizo la accion numero: 0\n","(4, 0.0, False, False, {'prob': 1.0})\n","Interacion numero: 3 hizo la accion numero: 1\n","(8, 0.0, False, False, {'prob': 1.0})\n","Interacion numero: 4 hizo la accion numero: 3\n","(4, 0.0, False, False, {'prob': 1.0})\n","Interacion numero: 5 hizo la accion numero: 2\n","(5, 0.0, True, False, {'prob': 1.0})\n","Discrete(16)\n"]}],"source":["from gym.envs.toy_text.frozen_lake import generate_random_map #Utilizar esto para generar mapas y evaluar al agente entrenado\n","\n","\n","def run():\n","    env = gym.make('FrozenLake-v1', desc=None, render_mode=\"human\" ,map_name=\"4x4\", is_slippery=False)\n","    \n","    lr = 0.6 # alpha / learning rate\n","    discount_factor = 0.7 # gamma / discount factor\n","    \n","    state = env.reset()[0] #Estados 0 al 15, 0=top left, 15=bottom left \n","    hoyo = False\n","    perdido = 200\n","    analisis = env.observation_space\n","    interacion = 0\n","\n","    while(not hoyo):\n","        action = env.action_space.sample() #acciones 0=izq,1=abajo,2=der,3=arriba\n","        info = env.step(action)\n","        new_state,reward,hoyo,_,_= info\n","    \n","        state = new_state\n","        interacion = interacion + 1\n","        \n","        print(\"Interaccion numero: \"+ str(interacion)+\" hizo la accion numero: \"+str(action))\n","        print(info)\n","        env.render()\n","        time.sleep(7)\n","        \n","    env.close()\n","    \n","    print(analisis)\n","run() #corre el Frozen Lake"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["#Qtable Printout\n","qtable = np.zeros((16,4))\n","print(qtable)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
