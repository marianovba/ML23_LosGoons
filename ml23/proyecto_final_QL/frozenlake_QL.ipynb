{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Proyecto Final ML23"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","import gymnasium as gym\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["# **Parametros**\n","\n","* `interacion:`       **int**   #Episodios\n","* `learning_rate:`    **float** #Learning rate\n","* `gamma:`            **float** #Discount rate\n","* `epsilon:`          **float** # Exploration probability\n","* `seed:`             **int**   # Define a seed so that we get reproducible results\n","* `n_runs:`           **int**   # Number of runs\n","* `action_size:`      **int**   # Number of possible actions\n","* `state_size:`       **int**   # Number of possible states\n","* `proba_frozen:`     **float** # Probability that a tile is frozen\n"," \n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#El hugo y sus parametros ://////\n","iteracion = 0\n","learning_rate = 0.6\n","intentos = 200 #numero de intentos\n","discount_rate = 0.5 #gamma\n","exploration_chance = 0.65 #epsilon\n","semilla = 56738\n","mininum_chance = 0.5\n","decreasing_decay = 0.1\n","rewards_per_iteracion = list()\n","env = gym.make('FrozenLake-v1', desc=None, render_mode=\"human\" ,map_name=\"4x4\", is_slippery=False) #ambiente"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class Qlearning:\n","    def _init_(self, learning_rate, gamma, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.learning_rate = learning_rate\n","        self.gamma = gamma\n","        self.reset_qtable()\n","    \n","    def update(self, state, action, reward, new_state):\n","        #Update Q(s,a) = Q(s,a) + Lr[ R(s,a) + gamma * maxQ(s',a') - Q(s,a)]\n","        delta = (\n","            reward\n","            + self.gamma *np.max(self.qtable[new_state, :])\n","            - self.qtable[state, action]\n","        )\n","        q_update = self.qtable[state, action] + self.learning_rate * delta\n","        return q_update\n","    \n","    def reset_qtable(self):\n","        #Resets Q-Table\n","        self.qtable=np.zeros((self.state_size,self.action_size))\n","\n","class EpsilonGreedy:\n","    def _init_(self, epsilon):\n","        self.epsilon = epsilon\n","\n","    def choose_action(self, action_space, state, qtable):\n","        \"Escoger una accion 'a' en el contexto actual (s)\"\n","        #Randomiza un numero\n","        explore_exploit_tradeoff = rng.uniform(0,1)\n","\n","        #Exploracion\n","        if(explore_exploit_tradeoff<self.epsilon):\n","            action = action_space.sample()\n","        \n","        #Explotacion (tomar el Q-valor mas grande para este estado)\n","        else:\n","            if np.all(qtable[state, :]) == qtable [state, 0]:\n","                action = action_space.sample()\n","            else:\n","                action = np.argmax(qtable[state,:])\n","        return action\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Interaccion numero: 1 hizo la accion numero: 3\n","(0, 0.0, False, False, {'prob': 1.0})\n","Interaccion numero: 2 hizo la accion numero: 0\n","(0, 0.0, False, False, {'prob': 1.0})\n","Interaccion numero: 3 hizo la accion numero: 3\n","(0, 0.0, False, False, {'prob': 1.0})\n","Interaccion numero: 4 hizo la accion numero: 1\n","(4, 0.0, False, False, {'prob': 1.0})\n","Interaccion numero: 5 hizo la accion numero: 0\n","(4, 0.0, False, False, {'prob': 1.0})\n","Interaccion numero: 6 hizo la accion numero: 0\n","(4, 0.0, False, False, {'prob': 1.0})\n","Interaccion numero: 7 hizo la accion numero: 2\n","(5, 0.0, True, False, {'prob': 1.0})\n","Discrete(16)\n"]}],"source":["from gym.envs.toy_text.frozen_lake import generate_random_map #Utilizar esto para generar mapas y evaluar al agente entrenado\n","\n","\n","def run():\n","\n","    state = env.reset()[0] #Estados 0 al 15, 0=top left, 15=bottom left \n","    hoyo = False\n","    analisis = env.observation_space\n","    interacion = 0\n","\n","    while(not hoyo):\n","        action = env.action_space.sample() #acciones 0=izq,1=abajo,2=der,3=arriba\n","        info = env.step(action)\n","        new_state,reward,hoyo,_,_= info\n","    \n","        state = new_state\n","        interacion = interacion + 1\n","        \n","        print(\"Interaccion numero: \"+ str(interacion)+\" hizo la accion numero: \"+str(action))\n","        print(info)\n","        env.render()\n","        time.sleep(3)\n","        \n","    env.close()\n","    \n","    print(analisis)\n","run() #corre el Frozen Lake"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["#Qtable Printout\n","qtable = np.zeros((16,4))\n","print(qtable)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["\n","#metodo para entrenar al agente \n","def training_waf():\n","    #Entrenamiento del agente imbezil\n","    #we iterate over episodes\n","    for e in range(iteracion):\n","    #we initialize the first state of the episode\n","        current_state = env.reset()\n","        done = False\n","    \n","     #sum the rewards that the agent gets from the environment\n","        total_episode_reward = 0\n","    \n","        for i in range(intentos): \n","        # we sample a float from a uniform distribution over 0 and 1\n","        # if the sampled flaot is less than the exploration proba\n","        #     the agent selects arandom action\n","        # else\n","        #     he exploits his knowledge using the bellman equation \n","        \n","            if np.random.uniform(0,1) < exploration_chance:\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(qtable[current_state,:])\n","        \n","            # The environment runs the chosen action and returns\n","            # the next state, a reward and true if the epiosed is ended.\n","            next_state, reward, done, _ = env.step(action)\n","        \n","            # We update our Q-table using the Q-learning iteration\n","            qtable[current_state, action] = (1-learning_rate) * qtable[current_state, action] +learning_rate*(reward + discount_rate*max(qtable[next_state,:]))\n","            total_episode_reward = total_episode_reward + reward\n","            # If the episode is finished, we leave the for loop\n","            if done:\n","                break\n","            current_state = next_state\n","    #We update the exploration proba using exponential decay formula \n","    exploration_chance = max(mininum_chance, np.exp(-decreasing_decay*e))\n","    rewards_per_iteracion.append(total_episode_reward)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
