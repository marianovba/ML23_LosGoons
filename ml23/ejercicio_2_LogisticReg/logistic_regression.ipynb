{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística\n",
    "En este ejercicio exploraremos el uso de Regresión logística. Primero programarás los elementos necesarios para optimizar la función por descenso de gradiente y después usaremos las librerias de scikit-learn para encontrar la solución. \n",
    "\n",
    "Regresión logística nos sirve para resolver problemas de <b>clasificación</b>, esto consiste en utilizar datos para estimar un valor discreto que será asignado a una categoría, en el siguiente código nos enfocaremos especificamente en clasificación \n",
    "binaria.\n",
    "\n",
    "Para entender mejor los conceptos utilizaremos el [\"Breast cancer wisconsin (diagnostic) dataset\"](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer). Este dataset contiene distintos atributos para determinar si un tumor es maligno(0) o benigno(1). Antes de empezar analizaremos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "# Podemos visualizar los atributos del dataset y la dimensionalidad de los datos\n",
    "print(\"Nobres de las columnas:\", dataset.keys())\n",
    "print(\"Matriz de datos (X.shape -> NxD): \", dataset['data'].shape)\n",
    "print(\"Cantidad de datos(N): \", dataset['target'].shape)\n",
    "print(\"Clases: 0-> %s, 1-> %s\" %(dataset.target_names[0], dataset.target_names[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos visualizar los nombres de los datos para decidir que atributos utilizar.\n",
    "# En este caso utilizaremos la textura y el perímetro del tumor para generar la predicción.\n",
    "print(dataset['feature_names'][[21, 22]])\n",
    "print(dataset['target_names']) \n",
    "\n",
    "# Haremos el split de los datos y a partir de ahora solo usaremos\n",
    "# el conjunto de entrenamiento para encontrar nuestro modelo\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    dataset['data'][:, [21, 22]], \n",
    "    dataset['target'], \n",
    "    test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Imprime la cantidad de puntos que serán dedicados para entrenamiento y\n",
    "# para prueba\n",
    "# ====== Start of solution =====\n",
    "print(f\"Entrenamiento - {...} datapoints\")\n",
    "print(f\"Evaluación - {...} datapoints\")\n",
    "# ====== End of solution =====\n",
    "\n",
    "# Es bueno saber como es la distribución de los datos\n",
    "# Incluyendo cuantos ejemplos son benignos y malignos\n",
    "malignant_idcs = np.where(target_train == 0)\n",
    "benign_idcs = np.where(target_train == 1)\n",
    "\n",
    "# TODO: Imprime la cantidad de puntos que son malignos y benignos\n",
    "# ====== Start of solution =====\n",
    "\n",
    "# ====== End of solution ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de datos\n",
    "Antes de resolver cualquier problema es necesario entenderlo, para los problemas de aprendizaje máquina una de las herramientas mas importantes para comprender mejor a que nos enfrentamos es el visualizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# Pintaremos los que tienen etiqueta 0 y etiqueta 1 de distinto color\n",
    "# Muestra un máximo de dp_to_plot puntos para cada clase\n",
    "dp_to_plot = 100\n",
    "\n",
    "# Grafica los datos malignos\n",
    "malignant_entries = data_train[malignant_idcs]\n",
    "ax.scatter(\n",
    "    # TODO: Ingresa los datos a graficar\n",
    "    # ====== Start of solution =====\n",
    "    x=...,\n",
    "    y=...,\n",
    "    # ====== End of solution =====\n",
    "    s=100,\n",
    "    color=\"r\", \n",
    "    marker=\"x\",\n",
    "    label=\"maligno\"\n",
    ")\n",
    "\n",
    "# Grafica los datos benignos\n",
    "benign_entries = data_train[benign_idcs]\n",
    "ax.scatter(\n",
    "    # TODO: Ingresa los datos a graficar\n",
    "    # ====== Start of solution =====\n",
    "\n",
    "    # ====== End of solution =====\n",
    "    s=100,\n",
    "    color=\"b\", \n",
    "    edgecolor='black',\n",
    "    marker=\"o\",\n",
    "    label=\"benigno\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función sigmoide\n",
    "Podemos aplicar un modelo de regresión logística para encontrar el límite de separación que nos permita clasificar nuevos tumores, a partir de los datos de entrenamiento.Recordamos la función logística vista en clase:\n",
    "\n",
    "$ h_w(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "En la siguiente celda, visualiza la función logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    # TODO: Calcula la hipótesis h(z)\n",
    "    # ====== Start of solution =====\n",
    "    h = ...\n",
    "    # ====== End of solution =====\n",
    "    return h\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = logistic(x)\n",
    "# TODO: Grafica la función logística\n",
    "# ====== Start of solution =====\n",
    "\n",
    "# ====== End of solution ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos aprender un modelo de regresión logística, debemos encontrar los parámetros w que minimizen la función de costo. En regresión logística, el modelo de decisión consiste en:\n",
    "\n",
    "$ h_w(X) = \\frac{1}{1 + e^{-Xw}}$\n",
    "\n",
    "Donde:\n",
    "- $X$ corresponde a los datos proporcionados\n",
    "- $h_w(X)$ es la predicción o hipótesis de nuestro modelo de regresión logística\n",
    "\n",
    "Recuerda que para realizar las operaciones de manera correcta $X$ debe estar en su notación aumentada, es decir, tiene un uno adicional por cada datapoint para que se pueda añadir el sesgo: $X \\in \\mathbb{R}^ {D+1 \\times N }$. Por lo tanto, los pesos tienen este parámetro adicional también $ w \\in R^{D+1} $\n",
    "\n",
    "En este problema $D = 2$ ya que estamos utilizando el perímetro y la textura del tumor para hacer la predición.\n",
    "\n",
    "TODO: En la siguiente celda, calcula las predicciones de regresión logística para una inicialización aleatoria de los pesos $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando W\n",
    "w = np.random.uniform(low=-1, high=1, size=(3,))\n",
    "\n",
    "# TODO: Genera el dataset de entrenamiento aumentado\n",
    "# para el conjunto de datos de entrenamiento data_train\n",
    "# aug_data_train = ?\n",
    "# ====== Start of solution =====\n",
    "aug_data_train = ...\n",
    "# ====== End of solution =====\n",
    "\n",
    "# TODO: Calcula el valor de z/ la combinación lineal de los pesos y datos de entrenamiento\n",
    "# ====== Start of solution =====\n",
    "z = ...\n",
    "# ====== End of solution =====\n",
    "pred = logistic(z)\n",
    "\n",
    "print(aug_data_train.shape)\n",
    "print(pred.shape)\n",
    "\n",
    "# Graficamos los puntos para visualizar la predicción de nuestro modelo actualmente\n",
    "plt.scatter(aug_data_train[:, 1],aug_data_train[:, 2], c=pred, cmap=\"RdYlGn\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de costo\n",
    "Como vemos, al inicializarse los pesos, el modelo clasifica la mayoría de los puntos como benignos(0), sin embargo una vez optimizada la función, el modelo aprende el valor de los pesos óptimos dados los datos de entrenamiento. Para optimizar los pesos necesitamos aplicar descenso de gradiente usando la función de costo.\n",
    "\n",
    "En clase vimos que la función de costo que se utiliza para problemas de clasificación binaria, se conoce como binary cross entropy loss (BCE)\n",
    "\n",
    "$ L(h_w(x), y) = - y \\log(h_w(x)) - (1 - y)\\log(1 - h_w(x)) $\n",
    "\n",
    "La funcion de logaritmo $\\log{x}$ tiende al infinito cuando $x=0$. Para evitar esto, es común agregar un valor muy pequeño $\\epsilon$ al valor dentro del logaritmo.\n",
    "\n",
    "$ L(h_w(x), y) = - y \\log(h_w(x) + \\epsilon) - (1 - y)\\log(1 - h_w(x) + \\epsilon) $\n",
    "\n",
    "Termina el método para calcular la función de costo correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(X, w, y):\n",
    "    \"\"\"\n",
    "        X: datos en notación aumentada,\n",
    "        w: pesos del modelo\n",
    "        y: etiquetas de los datos\n",
    "    \"\"\"\n",
    "    eps = 1e-5 \n",
    "    # TODO: Calcula la función de costo\n",
    "    # Nota - suma la variable eps dentro del logaritmo \n",
    "    # para evitar errores por números muy cercanos a log de 0\n",
    "    # ====== Start of solution =====\n",
    "    loss = ...\n",
    "    # ====== End of solution ======\n",
    "    return loss.mean(axis=0)\n",
    "\n",
    "loss = bce_loss(aug_data_train, w, target_train)\n",
    "print(\"Costo: %.3f\" %loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optmización\n",
    "Podemos resolver el problema anterior utilizando descenso por gradiente. Para ello, requeriríamos sacar el gradiente de la función de costo con respecto a los pesos e iterar como en el ejercicio de regresión lineal. Es decir, necesitaríamos el gradiente del BCE loss, por lo tanto no podemos usar la función anteriormente calculada ya que utilizamos un loss diferente (MSE).\n",
    "\n",
    "En esta ocasión resolveremos el problema con la librería de scikit-learn. Completa donde se indique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# TODO: Haz fit al modelo de regresión logística\n",
    "# usando los datos de entrenamiento\n",
    "# (data_train, target_train)\n",
    "# ====== Start of solution =====\n",
    "\n",
    "# ====== End of solution =====\n",
    "\n",
    "# TODO: Calcula las preddiciones\n",
    "# ====== Start of solution =====\n",
    "prediction = ...\n",
    "# ====== End of solution =====\n",
    "\n",
    "# TODO: Calcula la exactitud de tu predicción\n",
    "# en el set de entrenamiento (data_train, target_train)\n",
    "# Es decir, el promedio de datos correctamente clasificados\n",
    "# ====== Start of solution =====\n",
    "train_accuracy = ...\n",
    "# ====== End of solution =====\n",
    "\n",
    "# TODO: Calcula la probabilidad de que un tumor sea benigno\n",
    "# en el set de evaluación (data_test, target_test)\n",
    "# La dimensionalidad de prob debe ser (test_dp, )\n",
    "# Tip: Investiga la función \"LogisticRegression.predict_proba\"\n",
    "# ====== Start of solution =====\n",
    "prob = ...\n",
    "# ====== End of solution =====\n",
    "\n",
    "# TODO: Calcula la exactitud de tu predicción\n",
    "# en el set de evaluación (data_test, target_test)\n",
    "# Tip: Investiga la función \"LogisticRegression.score\"\n",
    "# ====== Start of solution =====\n",
    "test_accuracy = ...\n",
    "\n",
    "# ====== End of solution =====\n",
    "print(test_accuracy)\n",
    "\n",
    "# Graficamos los puntos para visualizar la predicción de nuestro modelo\n",
    "plt.scatter(data_test[:, 0],data_test[:, 1], c=prob, cmap=\"RdYlGn\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, calculamos una predicción para cada punto de evaluación. Específicamente, el color de cada punto indica una probabilidad de que el tumor correspondiente sea benigno. \n",
    "<b>TODO: Contesta las siguientes preguntas</b> \n",
    "- ¿Por qué es útil predecir la probabilidad de que el tumor sea benigno en lugar de decir con certeza qué tipo de tumor es? \n",
    "- Intuitivamente ¿Qué necesitarías para tener una predicción más acertada?\n",
    "- Menciona otro problema real que se pueda resolver con un modelo de clasificación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistemas_inteligentes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04dc998fdd71cb65825f35fa039c285a87c761883882ab18ec8c9090ce63cd9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
